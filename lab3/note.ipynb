{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f6928193d90>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=4, stride=2, padding=2)\n",
    "        self.norm1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=2)\n",
    "        self.norm2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=2)\n",
    "        self.norm3 = nn.BatchNorm2d(128)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear1 = nn.Linear(3200, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.norm1(x)\n",
    "        x = F.leaky_relu(x, 0.2)\n",
    "        x = self.conv2(x)\n",
    "        x = self.norm2(x)\n",
    "        x = F.leaky_relu(x, 0.2)\n",
    "        x = self.conv3(x)\n",
    "        x = self.norm3(x)\n",
    "        x = F.leaky_relu(x, 0.2)\n",
    "        x = self.flatten(x)\n",
    "        x = F.dropout(x, 0.5)\n",
    "        x = self.linear1(x)\n",
    "        x = F.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.linear1 = nn.Linear(64, 64 * 4 * 4)\n",
    "        self.conv1 = nn.ConvTranspose2d(64, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv2 = nn.ConvTranspose2d(64, 128, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv3 = nn.ConvTranspose2d(128, 256, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(256, 3, kernel_size=5, stride=1, padding=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = x.view(-1, 64, 4, 4)\n",
    "        x = self.conv1(x)\n",
    "        x = F.leaky_relu(x, 0.2)\n",
    "        x = self.conv2(x)\n",
    "        x = F.leaky_relu(x, 0.2)\n",
    "        x = self.conv3(x)\n",
    "        x = F.leaky_relu(x, 0.2)\n",
    "        x = self.conv4(x)\n",
    "        x = torch.tanh(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "discriminator = Discriminator().to(device)\n",
    "generator = Generator().to(device)\n",
    "discriminator_optimizer = torch.optim.Adam(discriminator.parameters(), lr=0.000001)\n",
    "generator_optimizer = torch.optim.Adam(generator.parameters(), lr=0.00001)\n",
    "\n",
    "# test_vectors = torch.randn(25, 64, device=device)\n",
    "# torch.save(test_vectors, 'test_vectors.pt')\n",
    "test_vectors = torch.load(\"test_vectors.pt\")\n",
    "\n",
    "y = generator.forward(torch.randn(64, device=device))\n",
    "plt.imshow(y[0].cpu().detach().numpy().transpose(1, 2, 0) / 2 + 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "def load_dataset():\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((32, 32)),\n",
    "            transforms.ToTensor(),\n",
    "            lambda x: x * 2 - 1,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    true_images = []\n",
    "    for f in os.listdir(\"crawled_cakes\"):\n",
    "        img = Image.open(os.path.join(\"crawled_cakes\", f))\n",
    "        img = transform(img)\n",
    "        true_images.append(img)\n",
    "\n",
    "    true_images = torch.stack(true_images)\n",
    "    true_images_dataset = true_images.to(device)\n",
    "    return true_images_dataset\n",
    "\n",
    "\n",
    "true_images_dataset = load_dataset()\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    true_images_dataset, batch_size=16, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Minimodel(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.lin = nn.Linear(2, 2)\n",
    "#         self.lin2 = nn.Linear(2, 2)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.lin(x)\n",
    "#         x = self.lin2(x)\n",
    "#         return x\n",
    "\n",
    "\n",
    "# minimodel = Minimodel()\n",
    "# input_val = torch.randn(10, 2)\n",
    "# loss_fn = lambda x: ((x - 42) ** 2).sum()\n",
    "# optimizer = torch.optim.SGD(minimodel.lin.parameters(), lr=0.01)\n",
    "\n",
    "# for i in range(20):\n",
    "#     optimizer.zero_grad()\n",
    "#     pred = minimodel(input_val)\n",
    "#     loss = loss_fn(pred)\n",
    "\n",
    "#     loss.backward()\n",
    "\n",
    "#     optimizer.step()\n",
    "\n",
    "#     print(pred)\n",
    "#     print(\"Loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_epoch = 0\n",
    "path = f\"models/epoch_{starting_epoch}\"\n",
    "# load model\n",
    "\n",
    "generator.load_state_dict(torch.load(os.path.join(path, \"generator.pt\")))\n",
    "discriminator.load_state_dict(torch.load(os.path.join(path, \"discriminator.pt\")))\n",
    "generator_optimizer.load_state_dict(\n",
    "    torch.load(os.path.join(path, \"generator_optimizer.pt\"))\n",
    ")\n",
    "discriminator_optimizer.load_state_dict(\n",
    "    torch.load(os.path.join(path, \"discriminator_optimizer.pt\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_batch(true_images):\n",
    "    true_length = true_images.shape[0]\n",
    "\n",
    "    generated_images = generator.forward(torch.randn(true_length, 64, device=device))\n",
    "    batch = torch.cat([true_images, generated_images])\n",
    "\n",
    "    labels = torch.cat([torch.ones(true_length), torch.zeros(true_length)]).to(device)\n",
    "    random_noise = torch.rand(2 * true_length, device=device) * 0.1 - 0.05\n",
    "    labels = labels + random_noise\n",
    "    labels = torch.max(labels, torch.zeros(2 * true_length, device=device))\n",
    "    labels = torch.min(labels, torch.ones(2 * true_length, device=device))\n",
    "    labels = torch.unsqueeze(labels, 1)\n",
    "    return batch, labels\n",
    "\n",
    "\n",
    "def monitor_and_save(epoch, discriminator_losses, generator_losses, modelpath=\"models\"):\n",
    "    path = f\"{modelpath}/epoch_{epoch}\"\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    os.makedirs(f\"{modelpath}/images\", exist_ok=True)\n",
    "    torch.save(generator.state_dict(), os.path.join(path, \"generator.pt\"))\n",
    "    torch.save(discriminator.state_dict(), os.path.join(path, \"discriminator.pt\"))\n",
    "    torch.save(\n",
    "        generator_optimizer.state_dict(), os.path.join(path, \"generator_optimizer.pt\")\n",
    "    )\n",
    "    torch.save(\n",
    "        discriminator_optimizer.state_dict(),\n",
    "        os.path.join(path, \"discriminator_optimizer.pt\"),\n",
    "    )\n",
    "\n",
    "    generated_images = generator.forward(test_vectors)\n",
    "    fig, ax = plt.subplots(5, 5, figsize=(10, 10))\n",
    "    for i in range(5):\n",
    "        for j in range(5):\n",
    "            ax[i, j].imshow(\n",
    "                generated_images[i * 5 + j].cpu().detach().numpy().transpose(1, 2, 0)\n",
    "                / 2\n",
    "                + 0.5\n",
    "            )\n",
    "            ax[i, j].axis(\"off\")\n",
    "\n",
    "    plt.savefig(os.path.join(path, \"generated_images.png\"))\n",
    "    plt.savefig(f\"{modelpath}/images/epoch_{epoch}.png\")\n",
    "    plt.close()\n",
    "\n",
    "    with open(os.path.join(path, \"losses.txt\"), \"w\") as f:\n",
    "        f.write(str(discriminator_losses))\n",
    "        f.write(\"\\n\")\n",
    "        f.write(str(generator_losses))\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Discriminator loss: 0.6270557\n",
      "Generator loss: 0.2821323\n",
      "Epoch 1\n",
      "Discriminator loss: 0.5582255\n",
      "Generator loss: 0.0955078\n",
      "Epoch 2\n",
      "Discriminator loss: 0.5061238\n",
      "Generator loss: 0.0609406\n",
      "Epoch 3\n",
      "Discriminator loss: 0.4573798\n",
      "Generator loss: 0.0409001\n",
      "Epoch 4\n",
      "Discriminator loss: 0.4130688\n",
      "Generator loss: 0.0308029\n",
      "Epoch 5\n",
      "Discriminator loss: 0.3800219\n",
      "Generator loss: 0.0240753\n",
      "Epoch 6\n",
      "Discriminator loss: 0.3481302\n",
      "Generator loss: 0.0193982\n",
      "Epoch 7\n",
      "Discriminator loss: 0.3204519\n",
      "Generator loss: 0.0159096\n",
      "Epoch 8\n",
      "Discriminator loss: 0.2920838\n",
      "Generator loss: 0.0141363\n",
      "Epoch 9\n",
      "Discriminator loss: 0.2711357\n",
      "Generator loss: 0.0126275\n",
      "Epoch 10\n",
      "Discriminator loss: 0.2531078\n",
      "Generator loss: 0.0103027\n",
      "Epoch 11\n",
      "Discriminator loss: 0.2354126\n",
      "Generator loss: 0.0094235\n",
      "Epoch 12\n",
      "Discriminator loss: 0.2220656\n",
      "Generator loss: 0.0085251\n",
      "Epoch 13\n",
      "Discriminator loss: 0.2081353\n",
      "Generator loss: 0.0076766\n",
      "Epoch 14\n",
      "Discriminator loss: 0.1948107\n",
      "Generator loss: 0.0067638\n",
      "Epoch 15\n",
      "Discriminator loss: 0.1886933\n",
      "Generator loss: 0.0062108\n",
      "Epoch 16\n",
      "Discriminator loss: 0.1791676\n",
      "Generator loss: 0.0058943\n",
      "Epoch 17\n",
      "Discriminator loss: 0.1699114\n",
      "Generator loss: 0.0055270\n",
      "Epoch 18\n",
      "Discriminator loss: 0.1620389\n",
      "Generator loss: 0.0050276\n",
      "Epoch 19\n",
      "Discriminator loss: 0.1569834\n",
      "Generator loss: 0.0046969\n",
      "Epoch 20\n",
      "Discriminator loss: 0.1480212\n",
      "Generator loss: 0.0043454\n",
      "Epoch 21\n",
      "Discriminator loss: 0.1457586\n",
      "Generator loss: 0.0042070\n",
      "Epoch 22\n",
      "Discriminator loss: 0.1402774\n",
      "Generator loss: 0.0040828\n",
      "Epoch 23\n",
      "Discriminator loss: 0.1338225\n",
      "Generator loss: 0.0036933\n",
      "Epoch 24\n",
      "Discriminator loss: 0.1301175\n",
      "Generator loss: 0.0034631\n",
      "Epoch 25\n",
      "Discriminator loss: 0.1243694\n",
      "Generator loss: 0.0033750\n",
      "Epoch 26\n",
      "Discriminator loss: 0.1237282\n",
      "Generator loss: 0.0031537\n",
      "Epoch 27\n",
      "Discriminator loss: 0.1185784\n",
      "Generator loss: 0.0030950\n",
      "Epoch 28\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/mjste/Desktop/Semestr_7/mro/lab3/note.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mjste/Desktop/Semestr_7/mro/lab3/note.ipynb#W3sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mjste/Desktop/Semestr_7/mro/lab3/note.ipynb#W3sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m discriminator_optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/mjste/Desktop/Semestr_7/mro/lab3/note.ipynb#W3sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m disc_losses\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39;49mitem())\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mjste/Desktop/Semestr_7/mro/lab3/note.ipynb#W3sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# train generator\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mjste/Desktop/Semestr_7/mro/lab3/note.ipynb#W3sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m batch \u001b[39m=\u001b[39m generator\u001b[39m.\u001b[39mforward(torch\u001b[39m.\u001b[39mrandn(\u001b[39m16\u001b[39m, \u001b[39m64\u001b[39m, device\u001b[39m=\u001b[39mdevice))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(3000):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    disc_losses = []\n",
    "    gen_losses = []\n",
    "    for ib, image_batch in enumerate(train_loader):\n",
    "        # train discriminator on true images\n",
    "        batch, labels = discriminator_batch(true_images=image_batch)\n",
    "        discriminator_optimizer.zero_grad()\n",
    "        pred = discriminator(batch)\n",
    "        loss = F.binary_cross_entropy(pred, labels)\n",
    "        loss.backward()\n",
    "        discriminator_optimizer.step()\n",
    "        disc_losses.append(loss.item())\n",
    "\n",
    "\n",
    "        # train discriminator on generated images\n",
    "\n",
    "\n",
    "        # train generator\n",
    "        batch = generator.forward(torch.randn(16, 64, device=device))\n",
    "        generator_optimizer.zero_grad()\n",
    "        pred = discriminator(batch)\n",
    "        target = torch.ones((pred.shape[0], 1), device=device)\n",
    "        loss = F.binary_cross_entropy(pred, target)\n",
    "        loss.backward()\n",
    "        generator_optimizer.step()\n",
    "        gen_losses.append(loss.item())\n",
    "\n",
    "    mean_disc_loss = sum(disc_losses) / len(disc_losses)\n",
    "    mean_gen_loss = sum(gen_losses) / len(gen_losses)\n",
    "    print(\"Discriminator loss: %.7f\" % mean_disc_loss)\n",
    "    print(\"Generator loss: %.7f\" % mean_gen_loss)\n",
    "\n",
    "    if (\n",
    "        epoch < 10\n",
    "        and epoch % 1 == 0\n",
    "        or epoch < 100\n",
    "        and epoch % 10 == 0\n",
    "        or epoch % 20 == 0\n",
    "    ):\n",
    "        monitor_and_save(epoch, mean_disc_loss, mean_gen_loss, modelpath=\"models_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'model_2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/mjste/Desktop/Semestr_7/mro/lab3/note.ipynb Cell 7\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mjste/Desktop/Semestr_7/mro/lab3/note.ipynb#X16sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     plt\u001b[39m.\u001b[39msavefig(\u001b[39m\"\u001b[39m\u001b[39mlosses.png\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mjste/Desktop/Semestr_7/mro/lab3/note.ipynb#X16sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     plt\u001b[39m.\u001b[39mshow()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/mjste/Desktop/Semestr_7/mro/lab3/note.ipynb#X16sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m visualize_losses()\n",
      "\u001b[1;32m/home/mjste/Desktop/Semestr_7/mro/lab3/note.ipynb Cell 7\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/mjste/Desktop/Semestr_7/mro/lab3/note.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvisualize_losses\u001b[39m(sourcepath\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmodel_2\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/mjste/Desktop/Semestr_7/mro/lab3/note.ipynb#X16sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     values \u001b[39m=\u001b[39m []\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/mjste/Desktop/Semestr_7/mro/lab3/note.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mfor\u001b[39;00m \u001b[39mdir\u001b[39m \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39;49mlistdir(sourcepath):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/mjste/Desktop/Semestr_7/mro/lab3/note.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mdir\u001b[39m\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mepoch_\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/mjste/Desktop/Semestr_7/mro/lab3/note.ipynb#X16sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m             \u001b[39mcontinue\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'model_2'"
     ]
    }
   ],
   "source": [
    "def visualize_losses(sourcepath=\"model_2\"):\n",
    "    values = []\n",
    "    for dir in os.listdir(sourcepath):\n",
    "        if not dir.startswith(\"epoch_\"):\n",
    "            continue\n",
    "        path = os.path.join(sourcepath, dir)\n",
    "        epoch = int(dir.split(\"_\")[1])\n",
    "\n",
    "        with open(os.path.join(path, \"losses.txt\"), \"r\") as f:\n",
    "            disc_loss = float(f.readline().split()[0])\n",
    "            gen_loss = float(f.readline().split()[0])\n",
    "\n",
    "        values.append((epoch, disc_loss, gen_loss))\n",
    "\n",
    "    values.sort(key=lambda x: x[0])\n",
    "    epochs = [x[0] for x in values]\n",
    "    disc_losses = [x[1] for x in values]\n",
    "    gen_losses = [x[2] for x in values]\n",
    "\n",
    "    fix, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "    ax.plot(epochs, disc_losses, label=\"Discriminator loss\")\n",
    "    ax.plot(epochs, gen_losses, label=\"Generator loss\")\n",
    "    ax.legend()\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    plt.savefig(\"losses.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
